{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading txt files \n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\MIT.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "MIT = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\Stanford.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "Stanford = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\Tesla.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "Tesla = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\\\UIS.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "UIS = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\\\UIUC.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "UIUC = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "filepath = \"D:\\\\study stuff\\\\UIC\\\\Sem - 2\\\\IDS 566 Text Analytics\\\\Assignment 1\\\\UIC.txt\"\n",
    "myfile = open(filepath,encoding=\"utf8\")\n",
    "UIC = myfile.read()\n",
    "myfile.close()\n",
    "\n",
    "#inputs = ['MIT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def processing(file):\n",
    "    \n",
    "    #Tokenizing the string object from the text file\n",
    "    tokens = word_tokenize(file)\n",
    "    \n",
    "    #Removing punctuations and making all letters to lower case\n",
    "    words = [w.lower() for w in tokens if w.isalpha()]\n",
    "    \n",
    "    #Removing the stopwords from the list of words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    #Stemming the list of words\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stemmed = [porter.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "    return filtered_tokens\n",
    "    \n",
    "    \n",
    "UIC = processing(UIC)\n",
    "MIT = processing(MIT)\n",
    "Stanford = processing(Stanford)\n",
    "Tesla = processing(Tesla)\n",
    "UIS = processing(UIS)\n",
    "UIUC = processing(UIUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24024790375501276 0.2070365358592693 0.17662650602409638 0.2197092084006462 0.14835456475583864\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "print(jaccard_similarity(UIC,UIUC),jaccard_similarity(UIC,UIS),jaccard_similarity(UIC,MIT),jaccard_similarity(UIC,Stanford),jaccard_similarity(UIC,Tesla)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the list of tokens into a string for vectorzier\n",
    "UIC      = ','.join(str(v) for v in UIC)\n",
    "UIUC     = ','.join(str(v) for v in UIUC)\n",
    "UIS      = ','.join(str(v) for v in UIS)\n",
    "MIT      = ','.join(str(v) for v in MIT)\n",
    "Stanford = ','.join(str(v) for v in Stanford)\n",
    "Tesla    = ','.join(str(v) for v in Tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.62673105 0.43889091 0.19983674 0.24679476 0.09550695]\n",
      " [0.62673105 1.         0.62364707 0.25943329 0.32945459 0.13006657]\n",
      " [0.43889091 0.62364707 1.         0.13009311 0.2010771  0.05377051]\n",
      " [0.19983674 0.25943329 0.13009311 1.         0.15492998 0.08718145]\n",
      " [0.24679476 0.32945459 0.2010771  0.15492998 1.         0.07887414]\n",
      " [0.09550695 0.13006657 0.05377051 0.08718145 0.07887414 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Calculating Cosine Similarity through TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc = [UIC,UIUC,UIS,MIT,Stanford,Tesla]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(doc)\n",
    "words = vectorizer.get_feature_names()\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "print(similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
